{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fb4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8ce906",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('cleaned_wine_training_data.csv')\n",
    "# the data is already cleaned, so we can just tokenize the words\n",
    "# tokenize the description data\n",
    "X['description'] = X['description'].apply(lambda x: word_tokenize(x))\n",
    "X.dropna(subset= ['description'], inplace=True)\n",
    "X.dropna(subset= ['price'], inplace=True)\n",
    "\n",
    "# train a word2vec model on the description data\n",
    "prelim = Word2Vec(X['description'], min_count=1, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcbe340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93964\n",
      "(36973, 100)\n",
      "36973\n"
     ]
    }
   ],
   "source": [
    "# how many words are in my word2vec model\n",
    "print(len(X['description']))\n",
    "print((prelim.wv.vectors.shape))\n",
    "print(len(prelim.wv))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cbc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have already transformed reviews into matrices\n",
    "# You might need to convert them into torch tensors before passing to DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b057e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5463ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [rich, ripe, blackberry, cassis, leathery, aro...\n",
      "2    [distinctive, dessert, wine, open, inky, dark,...\n",
      "3    [great, price, chardonnay, taste, like, cost, ...\n",
      "4    [grapefruit, lemon, star, anise, aroma, lead, ...\n",
      "5    [made, exclusively, pinot, noir, rich, highly,...\n",
      "Name: description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almoc\\AppData\\Local\\Temp\\ipykernel_4116\\3863145600.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  input_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in input_data_embeddings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded input data: torch.Size([93964, 75, 100])\n"
     ]
    }
   ],
   "source": [
    "input_data_embeddings = []\n",
    "max_length = 0\n",
    "\n",
    "print(X['description'].head())\n",
    "for review in X['description']:\n",
    "    embedding_sequence = [prelim.wv[word] for word in review if word in prelim.wv]\n",
    "    input_data_embeddings.append(embedding_sequence)\n",
    "    max_length = max(max_length, len(embedding_sequence))\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in input_data_embeddings]\n",
    "input_data_tensors_padded = pad_sequence(input_data_tensors, batch_first=True)\n",
    "\n",
    "# You can check the shape of input_data_tensors_padded to see the maximum length\n",
    "print(\"Shape of padded input data:\", input_data_tensors_padded.shape)\n",
    "\n",
    "# Assuming X[\"price\"] contains your target prices\n",
    "target_tensors = torch.tensor(X[\"price\"], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db5a8c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a CNN architecture that can handle variable length sequences\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class CNNVariableLength2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_filters, kernel_sizes, embedding_dim):\n",
    "        super(CNNVariableLength2, self).__init__()\n",
    "        \n",
    "        # self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # self.convs = nn.ModuleList([\n",
    "        #     nn.Conv1d(in_channels=embedding_dim, \n",
    "        #               out_channels=num_filters, \n",
    "        #               kernel_size=kernel_size)\n",
    "        #     for kernel_size in kernel_sizes\n",
    "        # ])\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=num_filters[0], kernel_size=kernel_sizes[0])\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_filters[0], out_channels=num_filters[1], kernel_size=kernel_sizes[1])\n",
    "        self.conv3 = nn.Conv2d(in_channels=num_filters[1], out_channels=num_filters[2], kernel_size=kernel_sizes[2])\n",
    "        # self.conv4 = nn.Conv2d(in_channels=num_filters[2], out_channels=num_filters[3], kernel_size=kernel_sizes[3])\n",
    "        # self.conv5 = nn.Conv2d(in_channels=num_filters[3], out_channels=num_filters[4], kernel_size=kernel_sizes[4])\n",
    "        # self.conv6 = nn.Conv2d(in_channels=num_filters[4], out_channels=num_filters[5], kernel_size=kernel_sizes[5])\n",
    "        \n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool2d(1)  # Global pooling\n",
    "        self.relu = nn.functional.relu\n",
    "\n",
    "        self.fc = nn.Linear(6, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        lengths = (x.any(dim=2) != 0).sum(dim=1)\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        \n",
    "        x = x[perm_idx]\n",
    "    \n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        x, _ = rnn_utils.pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "\n",
    "        # x = self.relu(self.conv4(x))\n",
    "\n",
    "        # x = self.relu(self.conv5(x))\n",
    "\n",
    "        # x = self.relu(self.conv6(x))\n",
    "\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        x = x.squeeze(2)\n",
    "     \n",
    "        x = x.squeeze(2)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "       \n",
    "\n",
    "        output = self.fc(x)  # [batch_size, output_dim]\n",
    "      \n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b60a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Assuming `data` and `labels` are lists of tensors\n",
    "# dataset = MyDataset(data, labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data, labels = zip(*batch)\n",
    "    data = pad_sequence(data, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    return data, labels\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6f401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, reps, criterion=nn.MSELoss()):\n",
    "        resps_np = reps.detach().numpy()\n",
    "        targets_np = targets.detach().numpy()\n",
    "        resps_np, targets_np = shuffle(resps_np, targets_np, random_state=0)\n",
    "        shuffled_reps = torch.tensor(resps_np, dtype=torch.float32)\n",
    "\n",
    "        shuffled_targets = torch.tensor(targets_np, dtype=torch.float32)\n",
    "        rep_loss = abs(criterion(reps, shuffled_reps) - criterion(targets, shuffled_targets))\n",
    "\n",
    "        loss = criterion(inputs, targets) + rep_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087ac526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_custom(model, data_loader, num_epochs=10, learning_rate=0.001):\n",
    "    print(\"TRAINING, learning rate:\", learning_rate)\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    crit = CustomLoss()\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs,rep = model(inputs)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = crit(outputs, labels, rep)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "     \n",
    "        loss_list.append(running_loss / len(data_loader))\n",
    "        print(epoch + 1, running_loss / len(data_loader))\n",
    "\n",
    "    print('Finished Training')\n",
    "    plt.plot(loss_list)\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62f51fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "    predictions = []\n",
    "    true_valus = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct10 = 0\n",
    "    total10 = 0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        inputs, labels = data\n",
    "        outputs,rep = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        for i in range(len(outputs)):\n",
    "            predictions.append(outputs[i])\n",
    "            true_valus.append(labels[i])\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        #add to correct if within 20% of the true price\n",
    "        correct += sum(abs(outputs - labels) < 0.2 * labels)\n",
    "        correct10 += sum(abs(outputs - labels) < 10 + labels)\n",
    "        total += len(labels)\n",
    "\n",
    "\n",
    "\n",
    "    loss_list.append(running_loss / len(data_loader))\n",
    "    print(\"Testing : \", running_loss / len(data_loader))\n",
    "    print(\"Correct: \", correct / total)\n",
    "    print(\"Correct10: \", correct10 / total)\n",
    "\n",
    "    print('Finished Testing')\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0a8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset and dataloader using the non-padded input data\n",
    "# batch_size = 32\n",
    "dataset = MyDataset(input_data_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "105b284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [made, cool, part, napa, valley, cool, vintage...\n",
      "1    [initial, aroma, boxwood, give, way, melon, cr...\n",
      "2    [smoky, leathery, barrel, flavor, soft, cabern...\n",
      "3    [ebullient, aroma, ripe, yellow, peach, tanger...\n",
      "4    [informal, red, open, aroma, suggest, darkskin...\n",
      "Name: description, dtype: object\n",
      "Shape of padded input data: torch.Size([22699, 72, 100])\n"
     ]
    }
   ],
   "source": [
    "X_val = pd.read_csv('cleaned_wine_validation_data.csv')\n",
    "X_val['description'] = X_val['description'].apply(lambda x: word_tokenize(x))\n",
    "X_val.dropna(subset= ['description'], inplace=True)\n",
    "X_val.dropna(subset= ['price'], inplace=True)\n",
    "\n",
    "\n",
    "input_data_embeddings = []\n",
    "max_length = 0\n",
    "\n",
    "print(X_val['description'].head())\n",
    "for review in X_val['description']:\n",
    "    embedding_sequence = [prelim.wv[word] for word in review if word in prelim.wv]\n",
    "    input_data_embeddings.append(embedding_sequence)\n",
    "    max_length = max(max_length, len(embedding_sequence))\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_data_tensors_val = [torch.tensor(seq, dtype=torch.float32) for seq in input_data_embeddings]\n",
    "input_data_tensors_padded_val = pad_sequence(input_data_tensors_val, batch_first=True)\n",
    "\n",
    "# You can check the shape of input_data_tensors_padded to see the maximum length\n",
    "print(\"Shape of padded input data:\", input_data_tensors_padded_val.shape)\n",
    "\n",
    "# Assuming X[\"price\"] contains your target prices\n",
    "target_tensors_val = torch.tensor(X_val[\"price\"], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e09b2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [aroma, recall, ripe, dark, berry, toast, whif...\n",
      "3    [catarratto, one, sicily, widely, farmed, whit...\n",
      "4    [right, starting, block, oaky, wine, dripping,...\n",
      "5    [fruity, lightly, herbaceous, fine, textured, ...\n",
      "6    [show, jellylike, flavor, orange, pear, earthy...\n",
      "Name: description, dtype: object\n",
      "Shape of padded input data: torch.Size([19983, 68, 100])\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv('cleaned_wine_testing_data.csv')\n",
    "X_test['description'] = X_test['description'].apply(lambda x: word_tokenize(x))\n",
    "X_test.dropna(subset= ['description'], inplace=True)\n",
    "X_test.dropna(subset= ['price'], inplace=True)\n",
    "\n",
    "\n",
    "input_data_embeddings = []\n",
    "max_length = 0\n",
    "\n",
    "print(X_test['description'].head())\n",
    "for review in X_test['description']:\n",
    "    embedding_sequence = [prelim.wv[word] for word in review if word in prelim.wv]\n",
    "    input_data_embeddings.append(embedding_sequence)\n",
    "    max_length = max(max_length, len(embedding_sequence))\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_data_tensors_test = [torch.tensor(seq, dtype=torch.float32) for seq in input_data_embeddings]\n",
    "input_data_tensors_padded_test = pad_sequence(input_data_tensors_test, batch_first=True)\n",
    "\n",
    "# You can check the shape of input_data_tensors_padded to see the maximum length\n",
    "print(\"Shape of padded input data:\", input_data_tensors_padded_test.shape)\n",
    "\n",
    "# Assuming X[\"price\"] contains your target prices\n",
    "target_tensors_test = torch.tensor(X_test[\"price\"], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c25747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset and dataloader using the non-padded input data\n",
    "# batch_size = 32\n",
    "dataset_val = MyDataset(input_data_tensors_val, target_tensors_val)\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=32, collate_fn=collate_fn)\n",
    "dataset_test = MyDataset(input_data_tensors_test, target_tensors_test)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f50fbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING, learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 2458/2937 [01:44<00:20, 23.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 15\u001b[0m train_custom(model, data_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m     18\u001b[0m l \u001b[38;5;241m=\u001b[39m test(model, data_loader_val)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m, in \u001b[0;36mtrain_custom\u001b[1;34m(model, data_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     16\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(data_loader)):\n\u001b[0;32m     18\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     19\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\almoc\\anaconda\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\almoc\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\almoc\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\almoc\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate_fn\u001b[39m(batch):\n\u001b[0;32m     18\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 19\u001b[0m     data \u001b[38;5;241m=\u001b[39m pad_sequence(data, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(labels)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data, labels\n",
      "File \u001b[1;32mc:\\Users\\almoc\\anaconda\\Lib\\site-packages\\torch\\nn\\utils\\rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    395\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [ .001,.01,.1,1,10,100]\n",
    "\n",
    "val_losses = []\n",
    "best_model = None\n",
    "best_loss = 1000000\n",
    "learn = 0\n",
    "\n",
    "for i in learning_rates:\n",
    "    model = CNNVariableLength2(input_dim=prelim.wv.vectors.shape[0], output_dim=1, num_filters=[10,8,6], kernel_sizes=[2,3,4], embedding_dim=prelim.wv.vectors.shape[1])\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=i)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_custom(model, data_loader, num_epochs=3, learning_rate=i)\n",
    "\n",
    "\n",
    "    l = test(model, data_loader_val)\n",
    "    if l < best_loss:\n",
    "        best_loss = l\n",
    "        best_model = model\n",
    "        learn = i\n",
    "    val_losses.append(l)\n",
    "\n",
    "print(\"Losses over Learning Rates: \", val_losses)\n",
    "print(\"Best Learning Rate = \", learn)\n",
    "print(\"Best Loss = \", best_loss)\n",
    "print(\"Best Model: \")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cc6d86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:49<00:00, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing :  1366.3885808810383\n",
      "Correct:  tensor(0.1938)\n",
      "Correct10:  tensor(0.8916)\n",
      "Finished Testing\n",
      "<class 'list'>\n",
      "1366.3885808810383\n"
     ]
    }
   ],
   "source": [
    "x = test(best_model, data_loader_val)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
